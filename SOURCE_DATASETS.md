# Dataset Attributions

This document provides citations for all datasets used in this project.

## Audio Datasets

### AudioCaps
Kim, C. D., Kim, B., Lee, H., & Kim, G. (2019). AudioCaps: Generating Captions for Audios in The Wild. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)* (pp. 119-132). Minneapolis, Minnesota: Association for Computational Linguistics.

### AudioSet
Gemmeke, J. F., Ellis, D. P. W., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., & Ritter, M. (2017). Audio Set: An ontology and human-labeled dataset for audio events. In *Proc. IEEE ICASSP 2017* (pp. 776-780). New Orleans, LA.

### VGGSound
Chen, H., Xie, W., Vedaldi, A., & Zisserman, A. (2020). VGGSound: A Large-scale Audio-Visual Dataset. In *ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)* (pp. 721-725).

### WavCaps
Mei, X., Meng, C., Liu, H., Kong, Q., Ko, T., Zhao, C., Plumbley, M. D., Zou, Y., & Wang, W. (2023). WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research. *arXiv preprint arXiv:2303.17395*.

## Image Datasets

### COCO (Common Objects in Context)
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., & Zitnick, C. L. (2014). Microsoft COCO: Common Objects in Context. In *Computer Vision – ECCV 2014* (pp. 740-755). Springer.

### Flickr30k
Young, P., Lai, A., Hodosh, M., & Hockenmaier, J. (2014). From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. *Transactions of the Association for Computational Linguistics*, 2, 67-78.

### Google Conceptual Captions
Sharma, P., Ding, N., Goodman, S., & Soricut, R. (2018). Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)* (pp. 2556-2565). Melbourne, Australia: Association for Computational Linguistics.

### ImageNet
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., & Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. *International Journal of Computer Vision*, 115(3), 211-252.

## Video Datasets

### InternVid
Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Chen, X., Wang, Y., Luo, P., Liu, Z., Wang, Y., Wang, L., & Qiao, Y. (2024). InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation. In *Proceedings of the International Conference on Learning Representations (ICLR)*.

### VidGen-1M
Tan, Z., Yang, X., Qin, L., & Li, H. (2024). VidGen-1M: A Large-Scale Dataset for Text-to-video Generation. *arXiv preprint arXiv:2408.02629*.

### VALOR
Chen, S., He, X., Guo, L., Zhu, X., Wang, W., Tang, J., & Liu, J. (2024). VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.

### VideoRefer
Yuan, Y., Zhang, H., Li, W., Cheng, Z., Zhang, B., Li, L., Li, X., Zhao, D., Zhang, W., Zhuang, Y., Zhu, J., & Bing, L. (2025). VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)* (pp. 18970-18980).


## 3D Datasets

### OpenShape
Liu, M., Shi, R., Kuang, K., Zhu, Y., Li, X., Han, S., Cai, H., Porikli, F., & Su, H. (2023). OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding. In *Advances in Neural Information Processing Systems (NeurIPS)*.


---

## Acknowledgments

We gratefully acknowledge the creators and maintainers of these datasets for their valuable contributions to the research community.